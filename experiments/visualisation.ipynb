{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def clean_and_parse_json(s):\n",
    "    try:\n",
    "        # First, try to parse the string as a Python literal (handles single quotes)\n",
    "        d = ast.literal_eval(s)\n",
    "        \n",
    "        # Convert to proper JSON format\n",
    "        # This ensures double quotes and lowercase booleans\n",
    "        return json.dumps({\n",
    "            \"message\": d[\"message\"],\n",
    "            \"toxicity\": str(d[\"toxicity\"]).lower() == \"true\",\n",
    "            \"reason\": d[\"reason\"]\n",
    "        })\n",
    "    except:\n",
    "        # If that fails, return a default JSON\n",
    "        return json.dumps({\n",
    "            \"message\": \"error\",\n",
    "            \"toxicity\": False,\n",
    "            \"reason\": f\"Error parsing prediction: {s[:100]}...\"\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message</th>\n",
       "      <th>game</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>keywords</th>\n",
       "      <th>location</th>\n",
       "      <th>model_prediction</th>\n",
       "      <th>predicted_toxicity</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wertz</td>\n",
       "      <td>du ma?</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>True</td>\n",
       "      <td>['du', 'ma']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>{'message': 'du ma?', 'toxicity': False, 'reas...</td>\n",
       "      <td>False</td>\n",
       "      <td>this message is not toxic as it does not conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kwangyy</td>\n",
       "      <td>troi oi!</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>False</td>\n",
       "      <td>['troi', 'oi']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>{'message': 'troi oi!', 'toxicity': False, 're...</td>\n",
       "      <td>False</td>\n",
       "      <td>this message does not contain any toxic keywor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wertz</td>\n",
       "      <td>how many nguyens we got in this server</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>True</td>\n",
       "      <td>['nguyen']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>{'message': 'how many nguyens we got in this s...</td>\n",
       "      <td>False</td>\n",
       "      <td>this comment is not toxic as it is a neutral q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>có thôi lời</td>\n",
       "      <td>gl hf</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>False</td>\n",
       "      <td>['gl', 'hf']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>{'message': 'gl hf', 'toxicity': False, 'reaso...</td>\n",
       "      <td>False</td>\n",
       "      <td>this is a positive comment that wishes good lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wertz</td>\n",
       "      <td>over under 3.5</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>{'message': 'over under 3.5', 'toxicity': Fals...</td>\n",
       "      <td>False</td>\n",
       "      <td>this message does not contain any toxic langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user                                 message            game  \\\n",
       "0        Wertz                                  du ma?  Counter-Strike   \n",
       "1      kwangyy                                troi oi!  Counter-Strike   \n",
       "2        Wertz  how many nguyens we got in this server  Counter-Strike   \n",
       "3  có thôi lời                                   gl hf  Counter-Strike   \n",
       "4        Wertz                          over under 3.5  Counter-Strike   \n",
       "\n",
       "   toxicity        keywords        location  \\\n",
       "0      True    ['du', 'ma']  Southeast Asia   \n",
       "1     False  ['troi', 'oi']  Southeast Asia   \n",
       "2      True      ['nguyen']  Southeast Asia   \n",
       "3     False    ['gl', 'hf']  Southeast Asia   \n",
       "4     False              []  Southeast Asia   \n",
       "\n",
       "                                    model_prediction  predicted_toxicity  \\\n",
       "0  {'message': 'du ma?', 'toxicity': False, 'reas...               False   \n",
       "1  {'message': 'troi oi!', 'toxicity': False, 're...               False   \n",
       "2  {'message': 'how many nguyens we got in this s...               False   \n",
       "3  {'message': 'gl hf', 'toxicity': False, 'reaso...               False   \n",
       "4  {'message': 'over under 3.5', 'toxicity': Fals...               False   \n",
       "\n",
       "                                              reason  \n",
       "0  this message is not toxic as it does not conta...  \n",
       "1  this message does not contain any toxic keywor...  \n",
       "2  this comment is not toxic as it is a neutral q...  \n",
       "3  this is a positive comment that wishes good lu...  \n",
       "4  this message does not contain any toxic langua...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the CSV\n",
    "base_predictions = pd.read_csv(\"results/base_prediction_results.csv\")\n",
    "\n",
    "# Clean and parse all predictions\n",
    "base_predictions['model_prediction'] = base_predictions['model_prediction'].apply(clean_and_parse_json)\n",
    "\n",
    "# Now we can safely parse them as JSON if needed\n",
    "base_predictions['model_prediction'] = base_predictions['model_prediction'].apply(json.loads)\n",
    "\n",
    "base_predictions[\"predicted_toxicity\"] = base_predictions[\"model_prediction\"].apply(lambda x: x[\"toxicity\"])\n",
    "\n",
    "base_predictions[\"reason\"] = base_predictions[\"model_prediction\"].apply(lambda x: x[\"reason\"])\n",
    "base_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message</th>\n",
       "      <th>game</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>keywords</th>\n",
       "      <th>location</th>\n",
       "      <th>extracted_keywords</th>\n",
       "      <th>used_definitions</th>\n",
       "      <th>model_prediction</th>\n",
       "      <th>predicted_toxicity</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wertz</td>\n",
       "      <td>du ma?</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>True</td>\n",
       "      <td>['du', 'ma']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>['du', 'ma']</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'message': 'du ma?', 'toxicity': False, 'reas...</td>\n",
       "      <td>False</td>\n",
       "      <td>this message does not contain any profanities,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kwangyy</td>\n",
       "      <td>troi oi!</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>False</td>\n",
       "      <td>['troi', 'oi']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'message': 'troi oi!', 'toxicity': False, 're...</td>\n",
       "      <td>False</td>\n",
       "      <td>this message does not contain any of the keywo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wertz</td>\n",
       "      <td>how many nguyens we got in this server</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>True</td>\n",
       "      <td>['nguyen']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>['nguyen']</td>\n",
       "      <td>{'nguyen': {'Southeast Asia': {'matched_word':...</td>\n",
       "      <td>{'message': 'how many nguyens we got in this s...</td>\n",
       "      <td>True</td>\n",
       "      <td>nguyen is being used as a derogatory term here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>có thôi lời</td>\n",
       "      <td>gl hf</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>False</td>\n",
       "      <td>['gl', 'hf']</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>['gl', 'hf']</td>\n",
       "      <td>{'gl': {'General': {'matched_word': 'gl', 'def...</td>\n",
       "      <td>{'message': 'gl hf', 'toxicity': False, 'reaso...</td>\n",
       "      <td>False</td>\n",
       "      <td>this is a positive comment that wishes good lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wertz</td>\n",
       "      <td>over under 3.5</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Southeast Asia</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'message': 'over under 3.5', 'toxicity': Fals...</td>\n",
       "      <td>False</td>\n",
       "      <td>this message does not contain any toxic keywor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user                                 message            game  \\\n",
       "0        Wertz                                  du ma?  Counter-Strike   \n",
       "1      kwangyy                                troi oi!  Counter-Strike   \n",
       "2        Wertz  how many nguyens we got in this server  Counter-Strike   \n",
       "3  có thôi lời                                   gl hf  Counter-Strike   \n",
       "4        Wertz                          over under 3.5  Counter-Strike   \n",
       "\n",
       "   toxicity        keywords        location extracted_keywords  \\\n",
       "0      True    ['du', 'ma']  Southeast Asia       ['du', 'ma']   \n",
       "1     False  ['troi', 'oi']  Southeast Asia                 []   \n",
       "2      True      ['nguyen']  Southeast Asia         ['nguyen']   \n",
       "3     False    ['gl', 'hf']  Southeast Asia       ['gl', 'hf']   \n",
       "4     False              []  Southeast Asia                 []   \n",
       "\n",
       "                                    used_definitions  \\\n",
       "0                                                 {}   \n",
       "1                                                 {}   \n",
       "2  {'nguyen': {'Southeast Asia': {'matched_word':...   \n",
       "3  {'gl': {'General': {'matched_word': 'gl', 'def...   \n",
       "4                                                 {}   \n",
       "\n",
       "                                    model_prediction  predicted_toxicity  \\\n",
       "0  {'message': 'du ma?', 'toxicity': False, 'reas...               False   \n",
       "1  {'message': 'troi oi!', 'toxicity': False, 're...               False   \n",
       "2  {'message': 'how many nguyens we got in this s...                True   \n",
       "3  {'message': 'gl hf', 'toxicity': False, 'reaso...               False   \n",
       "4  {'message': 'over under 3.5', 'toxicity': Fals...               False   \n",
       "\n",
       "                                              reason  \n",
       "0  this message does not contain any profanities,...  \n",
       "1  this message does not contain any of the keywo...  \n",
       "2  nguyen is being used as a derogatory term here...  \n",
       "3  this is a positive comment that wishes good lu...  \n",
       "4  this message does not contain any toxic keywor...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_predictions = pd.read_csv(\"results/complete_workflow_results.csv\")\n",
    "completed_predictions['model_prediction'] = completed_predictions['model_prediction'].apply(clean_and_parse_json)\n",
    "\n",
    "# Now we can safely parse them as JSON if needed\n",
    "completed_predictions['model_prediction'] = completed_predictions['model_prediction'].apply(json.loads)\n",
    "\n",
    "completed_predictions[\"predicted_toxicity\"] = completed_predictions[\"model_prediction\"].apply(lambda x: x[\"toxicity\"])\n",
    "\n",
    "completed_predictions[\"reason\"] = completed_predictions[\"model_prediction\"].apply(lambda x: x[\"reason\"])\n",
    "completed_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average keyword similarity: 0.687\n"
     ]
    }
   ],
   "source": [
    "# Calculate Jaccard score \n",
    "def calculate_jaccard(set1_str, set2_str):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity between two string representations of lists.\n",
    "    Returns score between 0 and 1, where 1 means perfect match.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert string representations of lists to actual sets\n",
    "        # Handle both string and list inputs\n",
    "        if isinstance(set1_str, str):\n",
    "            set1 = set(eval(set1_str))\n",
    "        else:\n",
    "            set1 = set(set1_str)\n",
    "            \n",
    "        if isinstance(set2_str, str):\n",
    "            set2 = set(eval(set2_str))\n",
    "        else:\n",
    "            set2 = set(set2_str)\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        \n",
    "        # Handle empty sets\n",
    "        if union == 0:\n",
    "            return 1.0 if len(set1) == len(set2) == 0 else 0.0\n",
    "            \n",
    "        return intersection / union\n",
    "    except:\n",
    "        return 0.0  # Return 0 for any parsing errors\n",
    "\n",
    "# Calculate Jaccard scores for the dataframe\n",
    "completed_predictions['keyword_similarity'] = completed_predictions.apply(\n",
    "    lambda row: calculate_jaccard(row['keywords'], row['extracted_keywords']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Print average similarity score\n",
    "print(f\"Average keyword similarity: {completed_predictions['keyword_similarity'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_classification_report(predictions):\n",
    "    # Get the actual toxicity values from the dataset\n",
    "    actual_toxicity = predictions['toxicity']\n",
    "\n",
    "    # Get the predicted toxicity values\n",
    "    predicted_toxicity = predictions['predicted_toxicity']\n",
    "\n",
    "    # Create the classification report\n",
    "    cr = classification_report(actual_toxicity, predicted_toxicity)\n",
    "\n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.88      0.92       122\n",
      "        True       0.62      0.86      0.72        28\n",
      "\n",
      "    accuracy                           0.87       150\n",
      "   macro avg       0.79      0.87      0.82       150\n",
      "weighted avg       0.90      0.87      0.88       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_cr = get_classification_report(base_predictions)\n",
    "\n",
    "print(base_cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.87      0.92       122\n",
      "        True       0.62      0.93      0.74        28\n",
      "\n",
      "    accuracy                           0.88       150\n",
      "   macro avg       0.80      0.90      0.83       150\n",
      "weighted avg       0.91      0.88      0.89       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completed_cr = get_classification_report(completed_predictions)\n",
    "\n",
    "print(completed_cr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
